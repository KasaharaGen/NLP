Q: Ripping a whole website to PDF Adobe Acrobat Pro allows the user to convert an entire website (or subset) to a PDF. Is there anything else available for OS X to do the same thing? I don't want to spend several hundred dollars.
Ideally, it would allow the user to rip a URL and all other URLs on the same "prefix" to a single PDF. For example, given the URL "http://example.com/a_web_page", it would pull "http://example.com/a_web_page/index.html", "http://example.com/a_web_page/a", "http://example.com/a_web_page/b", etc., but not "http://example.com/index.html" to a single PDF.
I have found whole books released under Creative Commons licenses as HTML pages (one per chapter, or section). I would like to capture the entire book to a single PDF to read on my iPad.
Thanks.

A: It doesn’t do “whole websites” because it would be hard for it to know “how deep” (tho I agree it could be configured in a way of: go down XX levels), but in any case, if your HTML pages contain a full chapter and assuming the books have about 20-30 chapters, is not that “bad”.
With that in mind, I have an inexpensive application that does this job, and it’s called Web Snapper. 

What's so special about Web Snapper?
Web Snapper is drag and drop simple.
  When you want to grab a page, you drag
  the URL and drop it onto the drop zone
  in Web Snapper (or the app icon in the
  dock) – it really could't be any
  simpler. With the 'Snapper, you can
  save pages as scalable/vector PDFs (we
  even preserve the links!), or in any
  of the image formats that are
  supported by MacOS X. If you want to
  make a multi-page PDF - simply drag
  and drop multiple URLs and then click
  the "Save to Multipage PDF".

It may help you because the price tag is $15.

A: Out company developed a batch process for this at PDFmyURL - with this you can convert an entire website to PDF by using a simple API. If you have the sitemap you can just pass this to the API and otherwise you pass the list of URLs that need to be crawled.
The details are here
